Policy Gradient可以说是最简单的RL算法，它直接对目标函数微分然后进行梯度下降。



## 1. 模型概述

![image-20240604165639222](./assets/image-20240604165639222.png)

考虑一个Policy $\pi$，这个policy定义了一个分布，能够在不同的状态下决定对操作的分配。

如果这个policy使用神经网络表示，那么$\theta$， policy的参数，会被神经网络权重所表示。



policy输出action之后，环境会根据状态转移概率返回状态，这个状态进入policy进行下一步的决策。



基于这个policy参数生成的**轨迹分布**我们由如下表示，即决策分布乘上概率转移分布，轨迹可以被简记为$\tau$
$$
p_\theta(\tau) = p_{\theta}(s_1,a_1,...,s_T,a_T) = p(s_1)\prod^T_{t=1}\pi_{\theta}(a_t|s_t)p(s_{t+1}|s_t,a_t)
$$

> 注意，在model free模型中，我们通常认为$p(s_1),p(s_{t+1}|s_t,a_t)$都是不可知的，我们只是假设能够与一个拥有这些分布的环境交互





我们的目的就是让当前轨迹分布下的轨迹期望奖励和最大
$$
\theta^* = \arg \max_\theta \color{blue}E_{\tau\sim p_\theta(\tau)}[\sum_t r(s_t,a_t)]
$$


> 在4.2中，我们提到了根据概率的链式法则以及马尔可夫性质，我们可以将期望奖励记为
> $$
> \color{blue} E_{\tau\sim p_\theta(\tau)}[\sum_t r(s_t,a_t)]  \color{black} = E_{s_1\sim p(s_1)}[E_{a_1\sim \pi(a_1|s_1)}[Q(s_1,a_1)|s_1]]
> $$
> 

> 这里的$Q(s_1,a_1)$代表$\color{red} 服从参数\theta的轨迹分布在s_1选取a_1到终态的期望总回报$
>
> 因此生成当前状态的最优动作可以根据$Q(s_t,a_t)$来获得
>
> 
>
> 在4.3中，我们知道了
>
> * $Q(s_t,a_t)$其实就是依据策略$\pi_\theta$​​​在轨迹上从当前步到最后能够获得的奖励总和
>   $$
>   Q^\pi(s_t,a_t) = \sum^T_{t'=t} E_{\pi_\theta}[r(s_{t'},a_{t'})|s_{t},a_{t}]
>   $$
>   此处，每个时间步的即时奖励$r(s_t,a_t)$事实上是基于前一步的状态$s_{t-1},a_{t-1}$的条件概率，但是如果写为$|s_{t'-1},a_{t'-1}$又太繁琐了，因此写为这个形式来代表"从$(s_t,a_t)$点开始往后的轨迹"。
>
>   
>
> 因此，我们可以进一步展开上面的式子
> $$
> \color{blue} E_{\tau\sim p_\theta(\tau)}[\sum_t r(s_t,a_t)]  \color{black} = E_{s_1\sim p(s_1)}[E_{a_1\sim \pi(a_1|s_1)}[Q(s_1,a_1)|s_1]]
>     \\ = E_{(s_1,a_1)\sim p_\theta(s_1,a_1)}[\sum^T_{t=1} E_{\pi_\theta}[r(s_{t},a_{t})|s_{1},a_{1}]]
>   
>     \\ =E_{(s_1,a_1)\sim p_\theta(s_1,a_1)}[r(s_1,a_1)] + \sum^T_{t=2} E_{\pi_\theta}[r(s_{t},a_{t})|s_{1},a_{1}]
>     \\=\sum^T_{t=1} E_{(s_t,a_t)\sim p_\theta(s_t,a_t)}[r(s_t,a_t)]
> $$
>
> * 值函数就是只知道状态，考虑动作的分布，在当前状态下的期望奖励
>
> $$
> V^\pi(s_t) \\= \sum^T_{t'=t}E_{\pi_\theta}[r(s_{t'},a_{t'})|s_t]\\=E_{a_t\sim\pi(a_t|s_t)}[Q^\pi(s_t,a_t)]
> $$
>
>   



因此，我们根据如上性质可以得到轨迹期望作为目标函数在**finite horizon**中为所有步上的即时奖励期望之和（记得每一步的期望受上一步的影响）
$$
\theta^* = \arg \max_\theta \sum^T_{t=1} E_{(s_t,a_t)\sim p_\theta(s_t,a_t)}[r(s_t,a_t)]
$$

在Chapter 3中提到，对于**Infinite horizon**，因为MDP的平稳分布原理，期望和将主要由平稳分布主导，因此期望奖励就为平稳分布下的单次奖励期望最大
$$
\theta^* = \arg \max_\theta E_{(s,a)\sim p_\theta(s,a)}[r(s,a)]
$$
这里的$p_\theta (s,a)$​就是平稳分布







## 2. Evaluate the objective

### 2.1 估计目标函数

$$
\theta^* = \arg\max_{\theta} \mathbb{E}_{\tau \sim p_{\theta}(\tau)} \left[ \sum_{t} r(s_t, a_t) \right]
$$

我们简记$\mathbb{E}_{\tau \sim p_{\theta}(\tau)} \left[ \sum_{t} r(s_t, a_t) \right]$为$J(\theta)$



因此
$$
\theta^* = \arg \max_\theta J(\theta)
$$


**现在我们需要考虑，如何在不知道$p(s_1)$与$p(s_{t+1}|s_t)$的情况下估计$J(\theta)$**

 

一个直接的方法就是，在具有这个分布的环境中采样若干次取平均，就能获得期望奖励$J(\theta)$的估计值
$$
J(\theta)=\mathbb{E}_{\tau \sim p_{\theta}(\tau)} \left[ \sum_{t} r(s_t, a_t) \right] \approx \frac{1}{N}\sum_i\sum_t r(s_{i,t},a_{i,t})
$$
这里，$s_{i,t}$代表采样的第$i$个trajectory的第$t$个时间步，总共采样$N$个

![image-20240605145033074](./assets/image-20240605145033074.png)

如图所示，这种方法采样若干个轨迹，这些轨迹有好有坏，采样的轨迹越多，对$J(\theta)$的估计就越准确。**因为采样时遵循环境中的概率分布，因此这个方法能够有效的估计期望奖励。**

> 这里，采样得到的回报均值被认为是**期望回报的无偏估计**
>
> ## 无偏估计
>
> 无偏估计（unbiased estimator）是统计学中的一个重要概念，用来描述估计量的性质。一个估计量$\hat{\theta}$如果**其期望值$\mathbb{E}[\hat{\theta}]$等于被估计的参数值$\theta$**，则称其为无偏估计量。
> $$
> \mathbb{E}[\hat{\theta}] = \theta
> $$
> 例如，我们对均值进行无偏估计：
>
> 假设我们有一组**独立同分布**的样本$X_1,X_2,..., X_n$， 来自一个总体，其均值为$\mu$，样本均值$\bar{X}=\frac{1}{n}\sum^n_{i=1}X_i$是总体均值$\mu$的无偏估计，因为
> $$
> \mathbb{E}[\bar{X}]=\mathbb{E}\left[ \frac{1}{n}\sum^n_{i=1} X_i\right]=\frac{1}{n}\sum^n_{i=1}\mathbb{E}[X_i] = \frac{1}{n}\sum^n_{i=1}\mu = \mu
> $$
> 无偏估计在统计推断中非常重要，因为它确保了估计量在长时间内不会系统性地偏离真实值。也就是说，使用无偏估计量进行估计，在重复试验中，其平均结果会趋近于真实的参数值。
>
> ## $J(\theta)$的无偏估计
>
> 在提供的$J(\theta)$估计方法中，我们估计值的期望就是我们要求的目标值（也是一个期望）
>
> * 每次采样都是独立同分布的(iid): 每次轨迹采样之间是独立的，并且都服从同样的分布$p_\theta(\tau)$，这样，**样本均值的期望**等于**总体的均值**，就是策略的真实期望回报
> * 采样得到的回报均值是期望回报的无偏估计，因为每次采样的轨迹都是独立且根据策略 $\theta$ 产生的。对于足够大的样本数量$N$，样本均值会趋近于期望值(大数法则)
>
> 数学上来说
>
> 对于每一条轨迹，回报的综合可以表示为
> $$
> G_i = \sum_t r(s_{i,t},a_{i,t})
> $$
> 其中$G_i$是第$i$条轨迹的总回报
>
> 
>
> 平均回报的估计为
> $$
> \hat{J}{(\theta)}=\frac{1}{N}\sum^N_{i=1}G_i
> $$
> 根据大数法则，当$N$很大时，$\hat{J}(\theta)$







我们接下来使用$r(\tau)$来表示轨迹奖励和$\color{red}r(\tau) = \sum^T_{t=1}r(s_t,a_t)$
$$
J(\theta) = E_{\tau\sim p_\theta(\tau)}[r(\tau)]
$$
当然考虑到概率分布是连续的，实际计算时是对所有可能轨迹$\tau$进行加总。由于轨迹的数量通常是无穷的，因此使用积分符号来表示。
$$
J(\theta) = \int p_\theta(\tau) r(\tau)d\tau
$$

> 其中$p_\theta(\tau)$是在策略 $\theta$ 下生成轨迹 $\tau$ 的概率密度函数。它表示在策略$\theta$下，生成特定轨迹的概率.
>
> 每个轨迹的回报$r(\tau)$ 按照其在策略$\theta$ 下的概率 $p_\theta(\tau)$进行加权。这相当于计算在策略 $\theta$下，轨迹回报的期望值。









### 2.2 评估梯度

$$
\triangledown_\theta J(\theta) = \int \triangledown_\theta p_\theta(\tau)r(\tau)d\tau
$$

这里因为梯度是线性的，因此可以直接放入积分内



因此，目标就变成了$p_\theta(\tau)$的关于$\theta$的梯度

考虑到$p_\theta$其实是未知的，所以需要一些聪明的数学变换来获得。根据log的求导公式可得
$$
\triangledown_\theta p_\theta(\tau)=p_\theta(\tau)\triangledown \log p_\theta(\tau) = p_\theta(\tau)\frac{\triangledown_\theta p_\theta(\tau)}{p_\theta(\tau)}
$$
这样，原公式就变为了
$$
\triangledown_\theta J(\theta) = \int \color{red}p_\theta(\tau)\color{blue}\triangledown \log p_\theta(\tau)r(\tau)\color{black}d\tau
$$
可以发现，这里的形式变成了一个轨迹概率分布$p_\theta(\tau)$乘上一个与轨迹相关的值$\triangledown \log p_\theta(\tau)r(\tau)$，因此可以看成是这个值的期望
$$
\triangledown_\theta J(\theta) =E_{\tau\sim p_\theta(\tau)}[\triangledown \log p_\theta(\tau)r(\tau)]
$$
注意，$\color{red}我们将它转换成了期望的形式，意味着我们可以使用sample来estimate这些期望$，因此这个方向实际上是正确的



考虑这个公式中的$\triangledown \log p_\theta(\tau)$
$$
p_\theta(s_1,a_1,...,s_T,a_T)=p(s_1)\prod^T_{t=1}\pi_\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)
$$

$$
\log p_\theta(\tau) = \log p(s_1) + \sum^T_{t=1}\log \pi_\theta(a_t|s_t) + \log p(s_{t+1}|s_t,a_t)
$$

因此
$$
\triangledown_\theta \log p_\theta(\tau) = 0 + \triangledown_\theta\sum^T_{t=1}\log \pi_\theta(a_t|s_t) + 0
$$
这样，$J(\theta)$就可以求得
$$
\begin{flalign}
  \triangledown_\theta J(\theta) 
&=E_{\tau\sim p_\theta(\tau)}[\triangledown \log p_\theta(\tau)r(\tau)]
\\&\color{red}= E_{\tau\sim p_\theta(\tau)}
\left[
\left( 
\sum^T_{t=1}\triangledown_\theta\log \pi_\theta(a_t|s_t)
\right)

\left(
\sum^T_{t=1}r(s_t,a_t)
\right)

\right]
\end{flalign}
$$
**现在，我们对于期望奖励$J(\theta)$中所有的参数都可以访问**

* 我们可以访问策略$\pi_\theta$: 神经网络
* 通过与环境交互，我们可以获得所有的$r(s_t,a_t)$









### 2.3 Overall Training process

根据上面的推导，我们将环境中的概率分布影响完全用采样得到的无偏估计消除，因此我们能够仅通过采样得到的若干trajectory来训练网络



1. 从环境中采样N个trajectory，获得当前策略$\pi_\theta$的无偏估计$\hat{J}(\theta)$
   $$
   J(\theta)=\mathbb{E}_{\tau \sim p_{\theta}(\tau)} \left[ \sum_{t} r(s_t, a_t) \right] \approx \hat{J}(\theta) = \frac{1}{N}\sum_i\sum_t r(s_{i,t},a_{i,t})
   $$

2. 我们证明了$\triangledown_\theta J(\theta)$只与policy $\pi_\theta$和每一步的$s_t,a_t$有关，因此我们可以**用每一步采样获得总梯度的无偏估计**
   $$
   \begin{flalign}
    \triangledown_\theta J(\theta) 
   &= E_{\tau\sim p_\theta(\tau)}
   \left[
   \left( 
   \sum^T_{t=1}\triangledown_\theta\log \pi_\theta(a_t|s_t)
   \right)
   
   \left(
   \sum^T_{t=1}r(s_t,a_t)
   \right)
   
   \right]
   
   \\
   &\approx \frac{1}{N}\sum^N_{i=1}
   \left(
   \sum^T_{t=1}\triangledown_\theta \log \pi_\theta(a_{i,t}|s_{i,t})
   \right)
   \left(
   \sum^T_{t=1}r(s_{i,t},a_{i,t})
   
   \right)
   
   \end{flalign}
   $$

3. 使用无偏估计的梯度来更新网络参数
   $$
   \theta \leftarrow \theta + \alpha\triangledown_\theta J(\theta)
   $$







### 2.4 期望奖励梯度的一个认知角度

考虑一个输入图像，输出左转右转的自动驾驶问题

![image-20240605202754476](./assets/image-20240605202754476.png)

在这里，$\log \pi_\theta(a_{i,t}|s_{i,t})$​​是**根据当前图像选择动作的对数概率**

在policy gradient中，目标是最大化期望奖励

在Imitation Learning中，我们的目标通常是最大化对应动作的对数概率

这两种模型都需要对所有trajectory的所有step都采样，他们的不同之处在于

* Policy gradient考虑奖励，而Imitation learning不考虑
* Policy gradient的trajectory质量层次不齐，而Imitation learning所用来训练的通常是专家动作/优质trajectory

因此Imitation Learning只会根据专家轨迹提升动作的概率，而Policy gradient会根据reward funciton 提升或降低动作的概率。

![image-20240605222106042](./assets/image-20240605222106042.png)

如图所示，Policy gradient会获得好的轨迹与坏的轨迹

它提升好的轨迹出现的概率，降低坏的轨迹出现的概率

> good stuff is made more likely, bad stuff is made less likely



具体来说，对于某一个trajectory的梯度
$$
\triangledown_\theta \log p_\theta(\tau) = \triangledown_\theta
\left(
\prod^T_{t=1}\pi_\theta(a_t|s_t) 
\right)

=\sum^T_{t=1}\triangledown_\theta \log \pi_\theta(a_t|s_t)
$$
更新参数时
$$
\theta \leftarrow \theta+\alpha \frac{1}{N}\sum^T_{t=1}\triangledown_\theta \log \pi_\theta(a_t|s_t)R(\tau)
$$
如果奖励是坏的，那么$\theta$就会下降，反之则会上升，这是一个较为简单的理解方向











### 2.5 连续动作下应用Policy gradient的一个例子

![image-20240605211115383](./assets/image-20240605211115383.png)

考虑我们想要使用策略梯度让这个小型人形机器人跑步。在这种情况下，我们需要选择一种表示 $\pi$​​​​ 的方法，使其能够输出连续值动作的分布。



首先，策略梯度定理为
$$
\begin{flalign}
 \triangledown_\theta J(\theta) 



&\approx \frac{1}{N}\sum^N_{i=1}
\left(
\sum^T_{t=1}\triangledown_\theta \log \pi_\theta(a_{i,t}|s_{i,t})
\right)
\left(
\sum^T_{t=1}r(s_{i,t},a_{i,t})

\right)

\end{flalign}
$$
要在连续动作空间中使用策略梯度，我们需要将$\pi_\theta(a_t|s_t)$表示为一个Multivariate normal distribution
$$
\pi_\theta(a_t|s_t) = \mathcal{N}(f_{neural\ network}(s_t);\Sigma)
$$
其中，均值由神经网络给出，$\Sigma$是协方差矩阵，可以是学习得到的也可以是固定的。



Multivariate Normal Distribution下的对数概率为
$$
\log \pi_\theta(a_t|s_t) = -\frac{1}{2}\Vert {f(s_t)-a_t}\Vert^2_\Sigma+const
$$
对均值参数求梯度我们就得到了
$$
\triangledown_\theta \log \pi_\theta(a_t|s_t) = -\frac{1}{2}\Sigma^{-1}(f(s_t)-a_t)\frac{df}{d\theta}
$$


> 多变量正态分布(Multivariate Normal Distribution)指的是多维空间中
>
> * 每个变量都服从正态分布
> * 变量之间存在某种相关性
>
> $$
> p(x) = \frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}exp\left(-\frac{1}{2}(\mathbf{x}-\mu)^T\Sigma^{-1}(\mathbf{x}-\mu)\right)
> $$
>
> 其中
>
> * $\mathbf{x}$是一个$k$ 维随机向量
> * $\mu$ 是一个$k$ 维均值向量
> * $\Sigma$ 是一个$k\times k$​的协方差矩阵
> * $|\Sigma|$表示协方差矩阵的行列式
>
> 
>
> 在强化学习中，连续动作空间意味着动作$a$是一个实数值的向量，而不是离散的有限集合，需要一种方法来生成和优化这些实数值的动作。
>
> 假设我们有一个二维空间中的动作向量$\mathbf{a}=[a_1,a_2]$​，其分布为一个二维正态分布，我们可以定义其均值向量和协方差矩阵如下，
> $$
> \mu = 
> \begin{bmatrix}
> \mu_1
> \\
> \mu_2
> \end{bmatrix}
> ,
> \Sigma = 
> \begin{bmatrix}
> \sigma_{11}&\sigma_{12}\\
> \sigma_{21}&\sigma_{22}\\
> \end{bmatrix}
> $$
> 在这个例子中，$\mu$表示动作的期望，而协方差矩阵$\Sigma$​表示动作之间的相关性和方差
>
> * **中心位置**：均值向量 $\mu$ 表示分布的中心，即最有可能出现的动作值。在强化学习中，这意味着在给定状态下，策略认为最优的动作是什么。
>
> * **期望动作**：在每个状态 $s_t$ 下，均值向量 $\mu$是神经网络输出的期望动作。这个动作是策略在当前状态下执行的“默认”动作。
>
> * **调节性**：通过调整均值向量，可以改变动作分布的中心，从而影响策略的行为。均值向量的学习是策略优化的一部分，使得策略能够适应不同的环境和任务。
>
> 
>
> **在策略梯度中**，我们使用神经网络来输出动作的均值$\mu$和协方差矩阵$\Sigma$。
>
> 对于动作$a_t$，其服从Multivariate normal distribution
> $$
> p(\mathbf{a}) = \frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}exp\left(-\frac{1}{2}(\mathbf{a}-\mu)^T\Sigma^{-1}(\mathbf{a}-\mu)\right)
> $$
> 对数概率密度为
> $$
> \log p(\mathbf{a}) = 
> \log
> \left(
> \frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}
> \right)
> +
> \log exp
> \left(
> -\frac{1}{2}(\mathbf{a}-\mu)^T\Sigma^{-1}(\mathbf{a}-\mu)
> \right)
> $$
> 考虑这里的$log$其实是$ln$，也就是$log(exp(x))=x$
> $$
> \log p(\mathbf{a}) =\log
> \left(
> \frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}
> \right)
> -\frac{1}{2}(\mathbf{a}-\mu)^T\Sigma^{-1}(\mathbf{a}-\mu)
> $$
> 考虑
> $$
> \log
> \left(
> \frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}
> \right)
> =\log 1 - \log(2\pi)^{k/2}-\log|\Sigma|^{1/2}
> $$
> 因此对数概率密度可以简化为
> $$
> \log p(\mathbf{a}) = -\frac{k}{2}\log(2\pi)-\frac{1}{2}\log|\Sigma| - \frac{1}{2}(\mathbf{a}-\mu)^T\Sigma^{-1}(\mathbf{a}-\mu)
> $$
> **这里的$p(\mathbf{a})$实际上就是$\pi_\theta({\mathbf{a}_t|s_t})$**
>
> 因此，每次采样获得的$(s,a)$对获得的对数期望奖励为
> $$
> \color{red}\log \pi_\theta(a_t|s_t) =-\frac{k}{2}\log(2\pi)-\frac{1}{2}\log|\Sigma| - \frac{1}{2}(\mathbf{a}_t-\mu)^T\Sigma^{-1}(\mathbf{a}_t-\mu)
> $$
> 这意味着我们可以根据$a_t$获得$\theta$​的梯度
> $$
> \triangledown_\theta\log \pi_\theta(a_t|s_t) =\triangledown_\theta \left(
> - \frac{1}{2}(\mathbf{a}_t-\mu)^T\Sigma^{-1}(\mathbf{a}_t-\mu)
> \right)
> $$
> 注意，这里我们考虑的是$\mu$是网络输出，$\Sigma$可能是网络输出，因此网络梯度就是$\triangledown_\theta\log \pi_\theta(a_t|s_t)$对与网络相关参数的偏导，



![image-20240605221823211](./assets/image-20240605221823211.png)















### 2.6 策略梯度下降的一个直观理解：如何指导网络优化

#### 2.6.1 策略梯度方法的基本原理

策略梯度方法的目标是通过最大化期望累积回报 $J(\theta)$ 来优化策略参数 $\theta$。期望累积回报可以表示为：

$$
J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)} \left[ \sum_{t=1}^T r(s_t, a_t) \right]
$$
为了优化 $J(\theta)$，我们需要计算其梯度 $\nabla_\theta J(\theta)$：

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)} \left[ \sum_{t=1}^T \nabla_\theta \log \pi_\theta(a_t | s_t) \sum_{t'=t}^T r(s_{t'}, a_{t'}) \right]
$$











#### 2.6.2 例子：采样的轨迹如何指导策略优化

通过采样得到的轨迹（即路径 \(\tau\)），可以有好有坏，这是由环境中的概率转移决定的。然而，通过策略梯度方法，我们能够利用这些采样结果来更新策略参数，使得未来采样的轨迹更有可能产生较高的回报。以下是一个具体的例子：

##### 示例：强化学习中的路径优化

假设我们有一个简单的环境，机器人需要从起点到达终点。每一步机器人可以选择向左或向右移动，每一步都有相应的回报。我们的目标是训练机器人找到一条最高回报的路径。

1. **策略定义**：
   假设策略 $\pi_\theta$ 是由一个简单的神经网络表示，输入是机器人的当前状态（位置），输出是左右移动的概率。

2. **采样轨迹**：
   我们从当前策略 $\pi_\theta$ 中采样多个轨迹。例如，采样到的三个轨迹可能如下：
   
   - 轨迹 1: $\tau_1 = \{(s_1, a_1), (s_2, a_2), \ldots, (s_T, a_T)\}$, 回报 $R_1 = 10$
   - 轨迹 2: $\tau_2 = \{(s_1, a_1), (s_2, a_2), \ldots, (s_T, a_T)\}$, 回报 $R_2 = 5$
   - 轨迹 3: $\tau_3 = \{(s_1, a_1), (s_2, a_2), \ldots, (s_T, a_T)\}$, 回报$(R_3 = 15$

3. **计算梯度**：
   对于每条轨迹，我们计算其对数概率的梯度，并乘以相应的回报。假设轨迹的对数概率梯度为 $\nabla_\theta \log p_\theta(\tau_i)$，梯度估计为：
   $$
   \nabla_\theta J(\theta) \approx \frac{1}{3} \left( \nabla_\theta \log p_\theta(\tau_1) R_1 + \nabla_\theta \log p_\theta(\tau_2) R_2 + \nabla_\theta \log p_\theta(\tau_3) R_3 \right)
   $$
   

   这个梯度的计算其实额外包括一系列复杂的运算
   
   * 在每一步获得$s_t,a_t$对，每次前向传播都会被记录$\log \pi_\theta(a_t|s_t)$
   * 读取池中的多个$s_t,a_t$对，取累积回报
   * 计算对数概率梯度，此时因为上述操作都会列入计算图，所以最终会分摊到每一个前向传播获得的$s_t,a_t$ 对的梯度，然后对神经网络进行反向传播
   
4. **更新策略参数**：
   我们使用梯度下降更新策略参数：
   $$
   \theta \leftarrow \theta + \alpha \nabla_\theta J(\theta)
   $$
   
5. 







##### 为什么采样的回报能够指导策略优化

- **高回报的轨迹贡献更大**：在梯度计算中，高回报的轨迹（如轨迹 3，回报 \(R_3 = 15\)）会有更大的权重，因为其梯度被更高的回报放大。这意味着这些轨迹对策略参数的更新贡献更大，使得策略更倾向于产生类似的高回报轨迹。
  
- **低回报的轨迹贡献较小**：相反，低回报的轨迹（如轨迹 2，回报 \(R_2 = 5\)）对策略参数的更新贡献较小，甚至可能减少这些轨迹的概率。这使得策略逐渐减少选择产生低回报轨迹的动作。

- **基于概率分布进行更新**：策略梯度方法通过采样得到的轨迹估计真实的期望回报的梯度。由于我们是从当前策略的概率分布中采样轨迹，这些轨迹在一定程度上反映了策略的性能。通过梯度更新，策略参数调整方向是增加产生高回报轨迹的概率，减少产生低回报轨迹的概率。





策略梯度方法通过对轨迹回报进行加权来指导策略优化。即使轨迹有好有坏，高回报的轨迹对策略参数的更新贡献更大，从而增加高回报轨迹的概率，减少低回报轨迹的概率。通过这种方式，策略逐渐优化，能够更有效地选择产生高回报的动作序列。







## 3.REINFORCE

经典的RINFORCE算法就是上面推到的一个直接体现

![image-20240605163919693](./assets/image-20240605163919693.png)























## 4. 部分观察下的Policy gradient

之前我们讨论的都是状态完全可观测的情况，也就是$s_t$

现在，以视觉自动驾驶为例，其实它是一个Observation

![image-20240605222520022](./assets/image-20240605222520022.png)

事实上，通过上面的推导，observation也能推导出来一模一样的式子，其实是因为概率分布隐含于环境的关系。



此时，我们发现，在推导期望奖励的这一个阶段，马尔可夫性质并没有确实的被使用，

$\color{red}因此我们可以在partially\ observed\ MDP中使用policy\ gradient，一点都不用修改$















## 5. 减少Policy gradient的方差

我们之前提出的Policy gradient实际上并没有应用马尔可夫性质，甚至不符合因果假设，因此我们将在此章节添加马尔可夫假设以及因果假设。

在策略梯度方法中，我们通过采样多个轨迹来估计策略梯度 $\triangledown_\theta J(\theta)$。由于这些轨迹是从策略 $\pi_\theta$ 中随机采样的，所以估计的梯度存在随机性。这个随机性带来的变化量就是梯度估计的方差。

在强化学习中的策略梯度方法中，"variance"（方差）通常指的是估计策略梯度的不确定性或波动性。**方差越小，意味着梯度估计越稳定，从而有助于更稳定和高效的策略优化。**

* 高方差的梯度估计会导致策略参数在更新过程中出现较大波动，从而影响策略的收敛性和优化效果。较低的方差有助于梯度估计的稳定性，使得策略优化过程更加平滑和高效。
* 高方差可能导致学习过程需要更多的样本和更新步骤来达到同样的性能。较低的方差则能够提高学习效率，减少训练时间和计算资源。



因此，我们希望$\triangledown_\theta J(\theta)$​是准确和稳定的，方差越小意味着它越接近真实梯度。

> 方差通常被计算为
> $$
> Var(X) = \mathbb{E}[(X-\mathbb E(X))^2]
> $$
> 或
> $$
> Var(X) = \int^∞_{-∞}(x-\mu)^2f(x)dx
> $$
> Multivariant Normal Distribution中，方差表现为协方差矩阵
> $$
> S=\frac{1}{n-1}(X-\bar X)^T(X-\bar X)
> $$
> $\bar X $是每个特征的均值向量





### 5.1 回顾

首先回顾一下，策略梯度方法的目标是找到能最大化期望累积回报的策略参数 θ\thetaθ。期望累积回报 $J(\theta)$ 定义为：
$$
J(\theta) = \mathbb{E}_{\tau\sim p_\theta(\tau)}

\left[
\sum^T_{t=1}r(s_t,a_t)


\right]
$$
这里，$\tau$表示从初始状态到终止状态的一条轨迹，为了优化$J(\theta)$，我们需要计算其梯度$\triangledown_\theta J(\theta)$
$$
\begin{flalign}
  \triangledown_\theta J(\theta) 
&=E_{\tau\sim p_\theta(\tau)}[\triangledown \log p_\theta(\tau)r(\tau)]
\\&\color{red}= E_{\tau\sim p_\theta(\tau)}
\left[
\left( 
\sum^T_{t=1}\triangledown_\theta\log \pi_\theta(a_t|s_t)
\right)

\left(
\sum^T_{t=1}r(s_t,a_t)
\right)

\right]
\end{flalign}
$$
这里，在自然推导中（2.2），我们采用log求导公式将不可知的$\log p_\theta$转换成了可知的$\log \pi_\theta$





### 5.2 因果假设

我们细看这个公式，它其实代表了**一个轨迹的梯度和**乘上**一个轨迹的奖励和**作为当前轨迹的梯度影响力。上面的式子考虑使用sample得到无偏估计
$$
\begin{flalign}
\triangledown_\theta J(\theta)&\approx\frac{1}{N}\sum^N_{i=1}

\left(

\sum^T_{t=1}\triangledown_\theta \log \pi_\theta(a_{i,t}|s_{i,t})

\right)

\left(
\sum^T_{t=1}r(s_{i,t},a_{i,t})
\right)

\\&=
\frac{1}{N}\sum^N_{i=1}



\sum^T_{t=1}\left(\triangledown_\theta \log \pi_\theta(a_{i,t}|s_{i,t})



\left(
\sum^T_{t'=1}r(s_{i,t'},a_{i,t'})
\right)
\right)

\end{flalign}
$$




事实上，**因果假设**告诉我们,$\color{red}在时间t'时的策略不能影响时间t<t'时的reward$

原本**每一步的策略的梯度获得的奖励加权是整个轨迹的奖励加权**，

现在根据因果假设，决策步$t'$的决策不会对其之前的奖励产生影响，这意味着决策步$t'$对于所有已经发生的决策步$t$的期望为0，在sample中我们认为，之前决策步获得奖励对当前决策步梯度的贡献应该为0.

因此，考虑了因果假设的期望奖励应该为
$$
\triangledown_\theta J(\theta)\approx\frac{1}{N}\sum^N_{i=1}



\sum^T_{t=1}\left(\triangledown_\theta \log \pi_\theta(a_{i,t}|s_{i,t})



\left(
\sum^T_{t'=t}r(s_{i,t'},a_{i,t'})
\right)
\right)
$$


其中$\sum^T_{t'=t}r(s_{i,t'},a_{i,t'})$通常被称为 reward to go，指的是从当前步到最后的reward，**也有人将其记为$\hat{Q}_{i,t}$**

这样，我们就获得了一个less variance的estimator



这里为什么说减少了方差呢，因为$\triangledown_\theta J(\theta)$本身的值因为这个改动一定会变小，那么不同采样获得的震荡相比原方法一定变小。

















### 5.3 Baseline

#### 5.3.1 Intuition

原本我们采样N条轨迹，梯度会为我们提高好轨迹的概率。

![image-20240606231442270](./assets/image-20240606231442270.png)

考虑这么一个情况，我们选择的所有轨迹都是好轨迹，只不过好的程度不一样

那么，我们的梯度就全是提升，这通常不会导向较好的结果，我们总是希望更好的轨迹拥有更高的概率。同时，采样得到全好的轨迹和采样获得全坏的轨迹这两种极端情况会导致梯度的方差很大。



很容易想到，我们可以设定一个base line，例如取轨迹奖励平均值为$b$，这样我们就能让最好的轨迹提升概率了。


$$
\triangledown_\theta J(\theta) \approx \frac{1}{N}\sum^N_{i=1} \triangledown_\theta \log p_\theta(\tau)[r(\tau)-b]
$$

#### 5.3.2 无偏估计

**那么，这么做是否会破坏无偏估计呢？**

答案是不会

因为期望的线性性质，我们可以仅仅考虑新添加进来的项的(使用log导数公式)
$$
\mathbb{E}[\triangledown_\theta \log p_\theta(\tau)b] = \int p_\theta(\tau) \triangledown_\theta \log p_\theta(\tau)b\ d\tau = \int \triangledown_\theta p_\theta(\tau)b\ d \tau = b\triangledown_\theta\int p_\theta(\tau)\ d\tau
$$
这里显然，根据归一化性质，对于所有可能的轨迹 $\tau$，它们的概率总和等于1
$$
\int p_\theta(\tau)d\tau=1
$$
因此
$$
\mathbb{E}[\triangledown_\theta \log p_\theta(\tau)b] = b\triangledown_\theta1 = b*0 = 0
$$
这意味着它不会改变梯度的期望**，满足了无偏估计**



#### 5.3.3 方差减少

**同时，梯度的方差是减少的**

同时，考虑$Var(X) = \mathbb{E}[(X-\mathbb E(X))^2]$，因为$X$本身降低了，**因此Var实际上是减少的。**





#### 5.3.4 计算最优Baseline

**并且，我们还可能计算出最优的baseline取代均值baseline**

我们首先推导一下方差，并根据方差来寻找$b$的最优值


$$
Var[x] = E[x^2]-E[x]^2
\\
\triangledown_\theta J(\theta) = E_{\tau\sim p_\theta(\tau)}[\triangledown_\theta \log p_\theta(\tau)(r(\tau)-b)]
\\
Var = E_{\tau\sim p_\theta(\tau)}[(\triangledown_\theta \log p_\theta(\tau)(r(\tau)-b))^2] - E_{\tau\sim p_\theta(\tau)}[\triangledown_\theta\log p_\theta(\tau)(r(\tau)-b)]^2
$$
这里，Var的第二项已经在5.3.2中被证明了与b无关

简记$\triangledown_\theta \log p_\theta(\tau)$为$g(\tau)$，则
$$
\begin{flalign}
\frac{dVar}{db} &= \frac{d}{db}E[g(\tau)^2(r(\tau)-b)^2]
\\
&= \frac{d}{db}
\left(
E[g(\tau)^2r(\tau)^2]  - 2E[g(\tau)^2r(\tau)b] + b^2E[g(\tau)^2]

\right)

\\
&= -2E[g(\tau)^2r(\tau)] + 2bE[g(\tau)^2]=0

\end{flalign}
$$
**注意只有标红项与b相关**

容易得到
$$
b = \frac{E[g(\tau)^2r(\tau)]}{E[g(\tau)^2]}
$$
这就是baseline的最优值。



这里可以发现，baseline取决于gradient，而gradient拥有很多参数，因此实际上就代表每个参数都决定着baseline。

同时，仔细观察上面的式子，可以发现，它实际上是用梯度加权的期望奖励取代了均值baseline。



因为计算复杂度的原因，通常均值baseline就已经足够了，通过训练量的提升能够有效的弥补baseline的不足。



















## 6. 拓展Policy gradient到off-policy

在Policy gradient中，我们计算期望奖励梯度时需要如下
$$
\triangledown_\theta J(\theta) = E_{\tau\sim p_\theta(\tau)}[\triangledown_\theta \log p_\theta(\tau)r(\tau)]
$$
这里，虽然我们利用采样规避了$p_\theta$的参数，但是我们仍然需要根据policy $\theta$从环境中采样轨迹。

这就导致了，一旦policy $\theta$ 改变了，那么我们必须扔掉样本，重新采样，**这一点让policy gradient成为一个on-policy的算法，它需要在每一步迭代fresh samples**， 我们不能保留数据的样本。





这就会导致两个问题

* 神经网络的每次梯度更新都是很微小的
* On-policy的效率因此十分低，或者代价十分昂贵









### 6.1 Importance Sampling

#### 6.1.1 Definition

Important Sampling是你希望通过**其他分布的采样**来评估**目标分布期望**的一种手段。

例如，为了评估$p(x)$分布下$f(x)$​的期望值，但是我们只能从$q(x)$中采样
$$
\begin{flalign}

E_{x\sim p(x)}[f(x)] &= \inf p(x)f(x)dx
\\
&= \int \frac{q(x)}{q(x)}p(x)f(x)dx
\\
&= \int q(x) \frac{p(x)}{q(x)}f(x)dx
\\
&= E_{x\sim q(x)}
\left[
\frac{p(x)}{q(x)}f(x)dx
\right]
\end{flalign}
$$
这个通过$q(x)$分布估计的**期望由数学保证一定不会变，而方差变大变小不确定**(取决于$\frac{p(x)}{q(x)}$大于1还是小于1)





#### 6.1.2 off-policy policy gradient

不难推导出，若$\bar{p}(\tau)$是以往采样时遵循的分布，$p_\theta(\tau)$是改变了$\theta$ 后的分布，那么
$$
J(\theta) = E_{\tau\sim\bar{p}(\tau)}
\left[
\frac{p_\theta(\tau)}{\bar p(\tau)}r(\tau)
\right]
$$

> **这里重要性采样具体代表了什么改动**
>
> 更直观的，我们可以以决策$\theta$ 下采样轨迹的概率分布为例说明
> $$
> p_\theta(\tau)=p(s_1)\prod^T_{t=1}\pi_\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)
> $$
> 考虑到其中环境初始分布$p(s_1)$和环境状态转移分布$p(s_{t+1}|s_t,a_t)$是不与决策网络相关的，因此它们是不变的
> $$
> \begin{flalign}
> \frac{p_\theta(\tau)}{\bar p(\tau)}
> &= \frac{p(s_1)\prod^T_{t=1}\pi_\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)}{p(s_1)\prod^T_{t=1}\bar \pi(a_t|s_t)p(s_{t+1}|s_t,a_t)}
> \\
> &= \color{red}\frac{\prod^T_{t=1}\pi_\theta(a_t|s_t)}{\prod^T_{t=1}\bar \pi(a_t|s_t)}
> \end{flalign}
> $$
> 所以它实际上是将旧决策的决策分布更改为了新决策的分布，以获得正确的决策期望。





##### 额外考虑因果性



**为了更清晰的阐述，我们记**

* **采样时使用的决策分布为$\theta$**
* **需要评估的新决策分布为$\theta'$**



因此，我们的问题模型为
$$
\theta^*=\arg\max_\theta J(\theta)
\\
J(\theta) = E_{\tau \sim p_\theta(\tau)}[r(\tau)]
\\
J(\theta') = E_{\tau\sim p_\theta(\tau)}
\left[

\frac{p_{\theta'}(\tau)}{p_\theta(\tau)}r(\tau)

\right]
$$
注意，在计算梯度时，我们需要的时当前决策分布$\theta'$ 的梯度
$$
\begin{flalign}

\triangledown_{\theta'}J(\theta') &= E_{\tau\sim p_\theta(\tau)}
\left[

\frac{\triangledown_{\theta'}p_{\theta'}(\tau)}{p_\theta(\tau)}r(\tau)

\right]

\\&= E_{\tau\sim p_\theta(\tau)}
\left[

\frac{p_{\theta'}(\tau)}{p_\theta(\tau)}
r(\tau)
\triangledown_{\theta'}\log p_{\theta'}(\tau)


\right]


\end{flalign}
$$
同样的，你可以通过2.2中提到的方法将之转换为关于$\pi_\theta$的梯度，并使用采样无偏估计来计算。
$$
\begin{flalign}

\triangledown_{\theta'}J(\theta')
&= E_{\tau\sim p_\theta(\tau)}
\left[

\frac{p_{\theta'}(\tau)}{p_\theta(\tau)}\triangledown_{\theta'}\log\pi_{\theta'}(\tau)r(\tau)\ 

\right]

\\
&= E_{\tau\sim p_\theta(\tau)}
\left[
\left(
\prod^T_{t=1}\frac{\pi_{\theta'}(a_t|s_t)}{\pi_\theta(a_t|s_t)}
\right)

\left(
\sum^T_{t=1}\triangledown_{\theta'}\log\pi_{\theta'}(a_t|s_t)
\right)

\left(
\sum^T_{t=1}r(s_t,a_t)
\right)

\right]

\\
&\overset{考虑因果性}{=}

\left[

\sum^T_{t=1}

\left(
\left(
\prod^t_{t'=1}\frac{\pi_{\theta'}(a_{t'}|s_{t'})}{\pi_\theta(a_{t'}|s_{t'})}
\right)
\triangledown_{\theta'}\log \pi_{\theta'}(a_t|s_t)
\right)


\left(
\sum^T_{t'=t}r(s_{t'},a_{t'})
\left(
\prod^{t'}_{t''=t}\frac{\pi_{\theta'}(a_{t''}|s_{t''})}{\pi_\theta(a_{t''}|s_{t''})}
\right)
\right)

\right]



\end{flalign}
$$
此处，Important Sampling的纠正因子对两个与$\theta$ 相关的参数指导修正

* 第一个就是基于当前决策参数的$\log$ 梯度分布$\triangledown_{\theta'}\log \pi_{\theta'}(a_t|s_t)$
  * future action don't affect current weight
  * **它需要考虑概率的链式传播，即从trajectory开始到当前决策步的联合修正**$\prod^t_{t'=1}\frac{\pi_{\theta'}(a_{t'}|s_{t'})}{\pi_\theta(a_{t'}|s_{t'})}$
* 第二个就是对未来期望奖励的修正，
  * **它需要考虑从当前决策步到trajectory结束的所有决策步联合修正**$\prod^{t'}_{t''=t}\frac{\pi_{\theta'}(a_{t''}|s_{t''})}{\pi_\theta(a_{t''}|s_{t''})}$

![image-20240607171505546](./assets/image-20240607171505546.png)



##### 提升：忽略第二个参数

$$
\triangledown_{\theta'}J(\theta')=\left[

\sum^T_{t=1}

\left(
\left(
\prod^t_{t'=1}\frac{\pi_{\theta'}(a_{t'}|s_{t'})}{\pi_\theta(a_{t'}|s_{t'})}
\right)
\triangledown_{\theta'}\log \pi_{\theta'}(a_t|s_t)
\right)


\left(
\sum^T_{t'=t}r(s_{t'},a_{t'})
\right)

\right]
$$







##### 进一步提升：改进指数级乘法

**考虑第二个修正，如果我们忽略这个参数，我们同样能改进策略梯度。**
$$
\triangledown_{\theta'}J(\theta')=\left[

\sum^T_{t=1}

\left(
\left(
\color{red}
\prod^t_{t'=1}\frac{\pi_{\theta'}(a_{t'}|s_{t'})}
{\pi_\theta(a_{t'}|s_{t'})}
\color{black}
\right)
\triangledown_{\theta'}\log \pi_{\theta'}(a_t|s_t)
\right)


\left(
\sum^T_{t'=t}r(s_{t'},a_{t'})
\right)

\right]
$$
首先，这个标红参数实际上是一个很麻烦的参数



考虑我们采样到了一个较差的trajectory，因为梯度下降的原因，它在最新的网络中概率一定是更低的

因此$\pi_{\theta'}(a_{t'}|s_{t'}) < \pi_\theta(a_{t'}|s_{t'})$

这使得$\frac{\pi_{\theta'}(a_{t'}|s_{t'})}{\pi_\theta(a_{t'}|s_{t'})} < 1$，当$T$足够大时，连乘无限趋近于0，**这进一步放大了梯度的方差**







为了简化公式，我们仍然用$\hat Q_{i,t}$来代表第i个trajectory第t步的未来奖励$\sum^T_{t'=t}r(s_{t'},a_{t'})$

在5.2 中，我们提到了 on-policy gradient
$$
\triangledown_\theta J(\theta) \approx \frac{1}{N}\sum^N_{i=1}\sum^T_{t=1}\triangledown_\theta \log \pi_\theta(a_{i,t}|s_{i,t})\hat Q_{i,t}
$$
这个公式同样可以被等效的看作从环境中依据当前策略采样N*T个状态动作对$(s_{i,t},a_{i,t})\sim \pi_\theta(s_t,a_t)$，而非是N个trajectory $\tau \sim p_\theta(\tau)$。

为什么能这么做呢，因为实际上，trajectory中某一步的状态分布与只考虑这一步的状态分布其实并没有区别。换句话说，==**策略执行过程中任意时间步的状态分布应该与该时间步单独考虑的状态分布一致**。==

因此，我们可以大胆的写出一个新的policy gradient，**它解除了重要性采样在条件概率上的链式联系**

换句话说，它不是在整个轨迹上进行重要性采样，而是在每一个$(s_t,a_t)$对上进行重要性采样，**这样就能摆脱指数级的重要性采样权重影响**
$$
\triangledown_\theta J(\theta) \approx \frac{1}{N}\sum^N_{i=1}\sum^T_{t=1}
\frac{\pi_{\theta'}(s_{i,t},a_{i,t})}{\pi_\theta(s_{i,t},a_{i,t})}
\triangledown_\theta \log \pi_\theta(a_{i,t}|s_{i,t})\hat Q_{i,t}
$$
同时，一个新的问题出现了，虽然我们能通过决策分布计算已知状态下的动作分布，

但是因为解除了概率链式联系，我们并**不知道环境中的状态的分布概率**$\pi_{\theta'}(s_{i,t}),\pi_{\theta}(s_{i,t})$，因为这个概率分布原本就是服务于trajectory的。
$$
\triangledown_\theta J(\theta) \approx \frac{1}{N}\sum^N_{i=1}\sum^T_{t=1}
\frac{\pi_{\theta'}(s_{i,t})}{\pi_\theta(s_{i,t})}
\frac{\pi_{\theta'}(a_{i,t}|s_{i,t})}{\pi_{\theta}(a_{i,t}|s_{i,t})}
\triangledown_\theta \log \pi_\theta(a_{i,t}|s_{i,t})\hat Q_{i,t}
$$




通常，为了解决$\prod^t_{t'=1}\frac{\pi_{\theta'}(a_{t'}|s_{t'})}{\pi_\theta(a_{t'}|s_{t'})}$，我们会忽略**状态的边缘分布**，以仅靠**动作在某一个状态下的边缘分布**指导重要性采样。
$$
\triangledown_\theta J(\theta) \approx \frac{1}{N}\sum^N_{i=1}\sum^T_{t=1}

\frac{\pi_{\theta'}(a_{i,t}|s_{i,t})}{\pi_{\theta}(a_{i,t}|s_{i,t})}
\triangledown_\theta \log \pi_\theta(a_{i,t}|s_{i,t})\hat Q_{i,t}
$$
可以发现，这个形式与之前的很像,区别就在于少了product

> $$
> \triangledown_{\theta'}J(\theta')=\left[
> 
> \sum^T_{t=1}
> 
> \left(
> \left(
> \color{red}
> \prod^t_{t'=1}\frac{\pi_{\theta'}(a_{t'}|s_{t'})}
> {\pi_\theta(a_{t'}|s_{t'})}
> \color{black}
> \right)
> \triangledown_{\theta'}\log \pi_{\theta'}(a_t|s_t)
> \right)
> 
> 
> \left(
> \sum^T_{t'=t}r(s_{t'},a_{t'})
> \right)
> 
> \right]
> $$



==虽然这通常不能提供正确的Important Sampling参数，但是在后文提及Advanced Policy Gradient时，忽略状态的边缘分布时合理的，因为它给出了**策略$\theta$与$\theta'$相差不是很大时有界的误差**==













































## 7. Implementing Policy Gradient

### 7.1 On policy

$$
\triangledown_\theta J(\theta)\approx \frac{1}{N}\sum^N_{i=1}\sum^T_{t=1}\triangledown_\theta \log \pi_\theta(a_{i,t}|s_{i,t})\hat Q_{i,t}
$$



传统的最大化似然如下
$$
maximum\ likelihood\ :
J_{ML}(\theta) \approx \frac{1}{N}\sum^N_{i=1}\sum^T_{t=1} \log \pi_\theta(a_{i,t}|s_{i,t})
\\
\triangledown_\theta J_{ML}(\theta) \approx\frac{1}{N}\sum^N_{i=1}\sum^T_{t=1} \triangledown_\theta \log \pi_\theta(a_{i,t}|s_{i,t})
$$
Pytorch的计算图保存了一个input $s_t$，output $a_t$的网络，这个最大化似然的梯度就是由pytorch的自动微分来backward



但是，我们实际上需要利用pytorch自动微分，对自动微分加权，换句话说就是为pytorch打一个补丁，来计算我们真正的loss，或者"pseudo-loss"

**Pytorch并没有实现这一部分，因为$\hat Q_{i,t}$本身受到我们策略的影响。**
$$
\tilde{J}(\theta)\approx \frac{1}{N}\sum^N_{i=1}\sum^T_{t=1}\log \pi_\theta(a_{i,t}|s_{i,t})\hat Q_{i,t}
$$
==这个等式不是强化学习的目标，或者可以说这个等式不是任何东西，它只是一个被选中的quantity，以使用它的微分作为policy gradient==



#### Maximum Likelihood 伪代码

以下是一个**最大化Likelihood**的伪代码

```python
# Given:
# actions -(N*T) x Da tensor of actions
# states - (N*T) x Ds tensor of states
# Build the graph

logits = policy.predictions(states) # return N*T x Da Tensor of action logits
negative_likelihoods = tf.nn.softmax_cross_entropy_with_logits(labels=actions, logits = logits)
loss = tf.reduce_mean(negative_likelihood)
gradients = loss.gradients(loss, variables)
```

1. 输入张量

   - `actions`：形状为 `(N*T) x Da` 的张量，其中 `N` 是轨迹数，`T` 是每个轨迹的时间步数，`Da` 是动作空间的维度。

   - `states`：形状为 `(N*T) x Ds` 的张量，其中 `Ds` 是状态空间的维度。

2. 预测动作
   - `logits = policy.predictions(states)`：通过策略网络 `policy` 对给定的 `states` 进行预测，返回形状为 `(N*T) x Da` 的动作 logits（未归一化的概率分布）。

3. 计算负对数似然损失

- ```python
  negative_likelihoods = tf.nn.softmax_cross_entropy_with_logits(labels=actions, logits=logits)
  ```

  ：使用 TensorFlow 的 `tf.nn.softmax_cross_entropy_with_logits` 函数计算交叉熵损失，`label`参数是实际的 `actions`，`logits`参数是策略网络预测的 logits。

  - `tf.nn.softmax_cross_entropy_with_logits` 函数计算的是负对数似然损失，这里不需要对 logits 进行 softmax，因为该函数会自动处理。
  - 这个函数会将 logits 通过 softmax 转换为概率分布，然后计算负对数似然损失。

4. 计算损失的平均值

   - `loss = tf.reduce_mean(negative_likelihoods)`：计算所有负对数似然损失的平均值，得到标量损失值。

   - 这个损失值反映了策略网络在当前策略下的性能。

5. 计算梯度

- `gradients = loss.gradients(loss, variables)`：计算损失 `loss` 相对于策略网络参数 `variables` 的梯度。`variables` 是策略网络中的可训练参数。































#### Policy gradient

**输入张量**：

- `actions`：形状为 `(N*T) x Da` 的张量，表示实际执行的动作。
- `states`：形状为 `(N*T) x Ds` 的张量，表示对应的状态。
- `q_values`：形状为 `(N*T) x 1` 的张量，表示估计的状态-动作值函数。

1. **预测动作**：
   - 通过策略网络 `policy` 的前向传播，根据给定的状态 `states` 预测动作的 logits。
2. **计算负对数似然损失**：
   - 使用 `tf.nn.softmax_cross_entropy_with_logits` 函数计算每个状态-动作对的交叉熵损失。这个函数会将 logits 通过 softmax 转换为概率分布，然后计算负对数似然损失。
3. **加权负对数似然损失**：
   - 将负对数似然损失与状态-动作值函数 QQQ 相乘，这相当于对每个状态-动作对的损失进行加权，使得高 QQQ 值的状态-动作对对损失的贡献更大。这种方法可以引导策略网络更关注那些高价值的状态-动作对。
4. **计算损失的平均值**：
   - `tf.reduce_mean` 函数对所有加权负对数似然损失取平均值，得到标量损失值。这个损失值反映了策略网络在当前策略下的加权性能。
5. **计算梯度**：
   - `tf.gradients` 函数计算损失 `loss` 相对于策略网络参数 `variables` 的梯度。 这些梯度用于更新策略网络的参数，以最小化加权损失，提高策略的性能。

```python
import tensorflow as tf

# 假设我们有一个策略网络 policy 和对应的变量 variables
policy = ...  # 你的策略网络
variables = ...  # 策略网络的可训练参数

# actions, states 和 q_values 是给定的输入张量
actions = ...  # 形状为 (N*T) x Da 的张量
states = ...  # 形状为 (N*T) x Ds 的张量
q_values = ...  # 形状为 (N*T) x 1 的张量

# 预测动作
logits = policy.predictions(states)  # 返回形状为 (N*T) x Da 的 logits 张量

# 计算负对数似然损失
negative_likelihoods = tf.nn.softmax_cross_entropy_with_logits(labels=actions, logits=logits)

# 加权负对数似然损失
weighted_negative_likelihoods = tf.multiply(negative_likelihoods, q_values)

# 计算损失的平均值
loss = tf.reduce_mean(weighted_negative_likelihoods)

# 计算梯度
gradients = tf.gradients(loss, variables)

```



















#### 一些注意点

1. 记住梯度具有高方差（Remember that the gradient has high variance）

   - 这与监督学习不同（This isn’t the same as supervised learning!）
     - 在监督学习中，我们通常有大量的标注数据，梯度计算相对稳定。然而，在策略梯度方法中，由于样本是通过策略采样获得的，样本的稀疏性和不确定性导致梯度的方差较大。

   - 梯度将会非常噪声（Gradients will be really noisy!）
     - 由于策略梯度方法依赖于样本轨迹，梯度估计会受到样本噪声的影响，导致梯度的不稳定。这会使得策略优化变得困难，因为梯度的高方差可能会导致策略更新方向不稳定。

2. 考虑使用更大的批量（Consider using much larger batches）
   - 使用较大的批量可以帮助减少梯度估计中的噪声。通过增加采样的轨迹数量，我们可以更好地平均化噪声，从而获得更稳定的梯度估计。这类似于在监督学习中使用更大的批量来平滑梯度估计。
   - 批量大小通常为100到1000不等。

3. 调整学习率非常困难（Tweaking learning rates is very hard）

   - 使用自适应步长规则（Adaptive step size rules like ADAM can be OK-ish）
     - 自适应优化算法（如Adam）可以帮助动态调整学习率，使得梯度更新更加稳定。Adam等自适应优化算法能够根据梯度的历史信息来调整学习率，从而缓解高方差对策略优化的影响。

   - 策略梯度有特定的学习率调整方法（We’ll learn about policy gradient-specific learning rate adjustment methods later!）
     - 在后续，我们将学习一些专门针对策略梯度方法的学习率调整技术。这些技术可能包括使用基线（baseline）减少方差、自然梯度（natural gradient）优化等。



























