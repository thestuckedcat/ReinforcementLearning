Policy Gradient可以说是最简单的RL算法，它直接对目标函数微分然后进行梯度下降。



## 1. 模型概述

![image-20240604165639222](./assets/image-20240604165639222.png)

考虑一个Policy $\pi$，这个policy定义了一个分布，能够在不同的状态下决定对操作的分配。

如果这个policy使用神经网络表示，那么$\theta$， policy的参数，会被神经网络权重所表示。



policy输出action之后，环境会根据状态转移概率返回状态，这个状态进入policy进行下一步的决策。



基于这个policy参数生成的**轨迹分布**我们由如下表示，即决策分布乘上概率转移分布，轨迹可以被简记为$\tau$
$$
p_\theta(\tau) = p_{\theta}(s_1,a_1,...,s_T,a_T) = p(s_1)\prod^T_{t=1}\pi_{\theta}(a_t|s_t)p(s_{t+1}|s_t,a_t)
$$

> 注意，在model free模型中，我们通常认为$p(s_1),p(s_{t+1}|s_t,a_t)$都是不可知的，我们只是假设能够与一个拥有这些分布的环境交互





我们的目的就是让当前轨迹分布下的轨迹期望奖励和最大
$$
\theta^* = \arg \max_\theta \color{blue}E_{\tau\sim p_\theta(\tau)}[\sum_t r(s_t,a_t)]
$$


> 在4.2中，我们提到了根据概率的链式法则以及马尔可夫性质，我们可以将期望奖励记为
> $$
> \color{blue} E_{\tau\sim p_\theta(\tau)}[\sum_t r(s_t,a_t)]  \color{black} = E_{s_1\sim p(s_1)}[E_{a_1\sim \pi(a_1|s_1)}[Q(s_1,a_1)|s_1]]
> $$
> 

> 这里的$Q(s_1,a_1)$代表$\color{red} 服从参数\theta的轨迹分布在s_1选取a_1到终态的期望总回报$
>
> 因此生成当前状态的最优动作可以根据$Q(s_t,a_t)$来获得
>
> 
>
> 在4.3中，我们知道了
>
> * $Q(s_t,a_t)$其实就是依据策略$\pi_\theta$​​​在轨迹上从当前步到最后能够获得的奖励总和
>   $$
>   Q^\pi(s_t,a_t) = \sum^T_{t'=t} E_{\pi_\theta}[r(s_{t'},a_{t'})|s_{t},a_{t}]
>   $$
>   此处，每个时间步的即时奖励$r(s_t,a_t)$事实上是基于前一步的状态$s_{t-1},a_{t-1}$的条件概率，但是如果写为$|s_{t'-1},a_{t'-1}$又太繁琐了，因此写为这个形式来代表"从$(s_t,a_t)$点开始往后的轨迹"。
>
>   
>
>     因此，我们可以进一步展开上面的式子
>   $$
>   \color{blue} E_{\tau\sim p_\theta(\tau)}[\sum_t r(s_t,a_t)]  \color{black} = E_{s_1\sim p(s_1)}[E_{a_1\sim \pi(a_1|s_1)}[Q(s_1,a_1)|s_1]]
>     \\ = E_{(s_1,a_1)\sim p_\theta(s_1,a_1)}[\sum^T_{t=1} E_{\pi_\theta}[r(s_{t},a_{t})|s_{1},a_{1}]]
>   
>     \\ =E_{(s_1,a_1)\sim p_\theta(s_1,a_1)}[r(s_1,a_1)] + \sum^T_{t=2} E_{\pi_\theta}[r(s_{t},a_{t})|s_{1},a_{1}]
>     \\=\sum^T_{t=1} E_{(s_t,a_t)\sim p_\theta(s_t,a_t)}[r(s_t,a_t)]
>   $$
>
> * 值函数就是只知道状态，考虑动作的分布，在当前状态下的期望奖励
>
> $$
> V^\pi(s_t) \\= \sum^T_{t'=t}E_{\pi_\theta}[r(s_{t'},a_{t'})|s_t]\\=E_{a_t\sim\pi(a_t|s_t)}[Q^\pi(s_t,a_t)]
> $$
>
>   



因此，我们根据如上性质可以得到轨迹期望作为目标函数在**finite horizon**中为所有步上的即时奖励期望之和（记得每一步的期望受上一步的影响）
$$
\theta^* = \arg \max_\theta \sum^T_{t=1} E_{(s_t,a_t)\sim p_\theta(s_t,a_t)}[r(s_t,a_t)]
$$

在Chapter 3中提到，对于**Infinite horizon**，因为MDP的平稳分布原理，期望和将主要由平稳分布主导，因此期望奖励就为平稳分布下的单次奖励期望最大
$$
\theta^* = \arg \max_\theta E_{(s,a)\sim p_\theta(s,a)}[r(s,a)]
$$
这里的$p_\theta (s,a)$​就是平稳分布







## 2. Evaluate the objective

### 2.1 估计目标函数

$$
\theta^* = \arg\max_{\theta} \mathbb{E}_{\tau \sim p_{\theta}(\tau)} \left[ \sum_{t} r(s_t, a_t) \right]
$$

我们简记$\mathbb{E}_{\tau \sim p_{\theta}(\tau)} \left[ \sum_{t} r(s_t, a_t) \right]$为$J(\theta)$



因此
$$
\theta^* = \arg \max_\theta J(\theta)
$$


**现在我们需要考虑，如何在不知道$p(s_1)$与$p(s_{t+1}|s_t)$的情况下估计$J(\theta)$**

 

一个直接的方法就是，在具有这个分布的环境中采样若干次取平均，就能获得期望奖励$J(\theta)$的估计值
$$
J(\theta)=\mathbb{E}_{\tau \sim p_{\theta}(\tau)} \left[ \sum_{t} r(s_t, a_t) \right] \approx \frac{1}{N}\sum_i\sum_t r(s_{i,t},a_{i,t})
$$
这里，$s_{i,t}$代表采样的第$i$个trajectory的第$t$个时间步，总共采样$N$个

![image-20240605145033074](./assets/image-20240605145033074.png)

如图所示，这种方法采样若干个轨迹，这些轨迹有好有坏，采样的轨迹越多，对$J(\theta)$的估计就越准确。**因为采样时遵循环境中的概率分布，因此这个方法能够有效的估计期望奖励。**

> 这里，采样得到的回报均值被认为是**期望回报的无偏估计**
>
> ## 无偏估计
>
> 无偏估计（unbiased estimator）是统计学中的一个重要概念，用来描述估计量的性质。一个估计量$\hat{\theta}$如果**其期望值$\mathbb{E}[\hat{\theta}]$等于被估计的参数值$\theta$**，则称其为无偏估计量。
> $$
> \mathbb{E}[\hat{\theta}] = \theta
> $$
> 例如，我们对均值进行无偏估计：
>
> 假设我们有一组**独立同分布**的样本$X_1,X_2,..., X_n$， 来自一个总体，其均值为$\mu$，样本均值$\bar{X}=\frac{1}{n}\sum^n_{i=1}X_i$是总体均值$\mu$的无偏估计，因为
> $$
> \mathbb{E}[\bar{X}]=\mathbb{E}\left[ \frac{1}{n}\sum^n_{i=1} X_i\right]=\frac{1}{n}\sum^n_{i=1}\mathbb{E}[X_i] = \frac{1}{n}\sum^n_{i=1}\mu = \mu
> $$
> 无偏估计在统计推断中非常重要，因为它确保了估计量在长时间内不会系统性地偏离真实值。也就是说，使用无偏估计量进行估计，在重复试验中，其平均结果会趋近于真实的参数值。
>
> ## $J(\theta)$的无偏估计
>
> 在提供的$J(\theta)$估计方法中，我们估计值的期望就是我们要求的目标值（也是一个期望）
>
> * 每次采样都是独立同分布的(iid): 每次轨迹采样之间是独立的，并且都服从同样的分布$p_\theta(\tau)$，这样，**样本均值的期望**等于**总体的均值**，就是策略的真实期望回报
> * 采样得到的回报均值是期望回报的无偏估计，因为每次采样的轨迹都是独立且根据策略 $\theta$ 产生的。对于足够大的样本数量$N$，样本均值会趋近于期望值(大数法则)
>
> 数学上来说
>
> 对于每一条轨迹，回报的综合可以表示为
> $$
> G_i = \sum_t r(s_{i,t},a_{i,t})
> $$
> 其中$G_i$是第$i$条轨迹的总回报
>
> 
>
> 平均回报的估计为
> $$
> \hat{J}{(\theta)}=\frac{1}{N}\sum^N_{i=1}G_i
> $$
> 根据大数法则，当$N$很大时，$\hat{J}(\theta)$



我们接下来使用$r(\tau)$来表示轨迹奖励和$\color{red}r(\tau) = \sum^T_{t=1}r(s_t,a_t)$
$$
J(\theta) = E_{\tau\sim p_\theta(\tau)}[r(\tau)]
$$
当然考虑到概率分布是连续的，实际计算时是对所有可能轨迹$\tau$进行加总。由于轨迹的数量通常是无穷的，因此使用积分符号来表示。
$$
J(\theta) = \int p_\theta(\tau) r(\tau)d\tau
$$

> 其中$p_\theta(\tau)$是在策略 $\theta$ 下生成轨迹 $\tau$ 的概率密度函数。它表示在策略$\theta$下，生成特定轨迹的概率.
>
> 每个轨迹的回报$r(\tau)$ 按照其在策略$\theta$ 下的概率 $p_\theta(\tau)$进行加权。这相当于计算在策略 $\theta$下，轨迹回报的期望值。









### 2.2 评估梯度

$$
\triangledown_\theta J(\theta) = \int \triangledown_\theta p_\theta(\tau)r(\tau)d\tau
$$

这里因为梯度是线性的，因此可以直接放入积分内



因此，目标就变成了$p_\theta(\tau)$的关于$\theta$的梯度

考虑到$p_\theta$其实是未知的，所以需要一些聪明的数学变换来获得。根据log的求导公式可得
$$
\triangledown_\theta p_\theta(\tau)=p_\theta(\tau)\triangledown \log p_\theta(\tau) = p_\theta(\tau)\frac{\triangledown_\theta p_\theta(\tau)}{p_\theta(\tau)}
$$
这样，原公式就变为了
$$
\triangledown_\theta J(\theta) = \int \color{red}p_\theta(\tau)\color{blue}\triangledown \log p_\theta(\tau)r(\tau)\color{black}d\tau
$$
可以发现，这里的形式变成了一个轨迹概率分布$p_\theta(\tau)$乘上一个与轨迹相关的值$\triangledown \log p_\theta(\tau)r(\tau)$，因此可以看成是这个值的期望
$$
\triangledown_\theta J(\theta) =E_{\tau\sim p_\theta(\tau)}[\triangledown \log p_\theta(\tau)r(\tau)]
$$
注意，$\color{red}我们将它转换成了期望的形式，意味着我们可以使用sample来estimate这些期望$，因此这个方向实际上是正确的



考虑这个公式中的$\triangledown \log p_\theta(\tau)$
$$
p_\theta(s_1,a_1,...,s_T,a_T)=p(s_1)\prod^T_{t=1}\pi_\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)
$$

$$
\log p_\theta(\tau) = \log p(s_1) + \sum^T_{t=1}\log \pi_\theta(a_t|s_t) + \log p(s_{t+1}|s_t,a_t)
$$

因此
$$
\triangledown_\theta \log p_\theta(\tau) = 0 + \triangledown_\theta\sum^T_{t=1}\log \pi_\theta(a_t|s_t) + 0
$$
这样，$J(\theta)$就可以求得
$$
\begin{flalign}
  \triangledown_\theta J(\theta) 
&=E_{\tau\sim p_\theta(\tau)}[\triangledown \log p_\theta(\tau)r(\tau)]
\\&\color{red}= E_{\tau\sim p_\theta(\tau)}
\left[
\left( 
\sum^T_{t=1}\triangledown_\theta\log \pi_\theta(a_t|s_t)
\right)

\left(
\sum^T_{t=1}r(s_t,a_t)
\right)

\right]
\end{flalign}
$$
**现在，我们对于期望奖励$J(\theta)$中所有的参数都可以访问**

* 我们可以访问策略$\pi_\theta$: 神经网络
* 通过与环境交互，我们可以获得所有的$r(s_t,a_t)$









### 2.3 Overall Training process

根据上面的推导，我们将环境中的概率分布影响完全用采样得到的无偏估计消除，因此我们能够仅通过采样得到的若干trajectory来训练网络



1. 从环境中采样N个trajectory，获得当前策略$\pi_\theta$的无偏估计$\hat{J}(\theta)$
   $$
   J(\theta)=\mathbb{E}_{\tau \sim p_{\theta}(\tau)} \left[ \sum_{t} r(s_t, a_t) \right] \approx \hat{J}(\theta) = \frac{1}{N}\sum_i\sum_t r(s_{i,t},a_{i,t})
   $$

2. 我们证明了$\triangledown_\theta J(\theta)$只与policy $\pi_\theta$和每一步的$s_t,a_t$有关，因此我们可以**用每一步采样获得总梯度的无偏估计**
   $$
   \begin{flalign}
    \triangledown_\theta J(\theta) 
   &= E_{\tau\sim p_\theta(\tau)}
   \left[
   \left( 
   \sum^T_{t=1}\triangledown_\theta\log \pi_\theta(a_t|s_t)
   \right)
   
   \left(
   \sum^T_{t=1}r(s_t,a_t)
   \right)
   
   \right]
   
   \\
   &\approx \frac{1}{N}\sum^N_{i=1}
   \left(
   \sum^T_{t=1}\triangledown_\theta \log \pi_\theta(a_{i,t}|s_{i,t})
   \right)
   \left(
   \sum^T_{t=1}r(s_{i,t},a_{i,t})
   
   \right)
   
   \end{flalign}
   $$

3. 使用无偏估计的梯度来更新网络参数
   $$
   \theta \leftarrow \theta + \alpha\triangledown_\theta J(\theta)
   $$







### 2.4 期望奖励梯度的一个认知角度

考虑一个输入图像，输出左转右转的自动驾驶问题

![image-20240605202754476](./assets/image-20240605202754476.png)

在这里，$\log \pi_\theta(a_{i,t}|s_{i,t})$​​是**根据当前图像选择动作的对数概率**

在policy gradient中，目标是最大化期望奖励

在Imitation Learning中，我们的目标通常是最大化对应动作的对数概率

这两种模型都需要对所有trajectory的所有step都采样，他们的不同之处在于

* Policy gradient考虑奖励，而Imitation learning不考虑
* Policy gradient的trajectory质量层次不齐，而Imitation learning所用来训练的通常是专家动作/优质trajectory

因此Imitation Learning只会根据专家轨迹提升动作的概率，而Policy gradient会根据reward funciton 提升或降低动作的概率。

![image-20240605222106042](./assets/image-20240605222106042.png)

如图所示，Policy gradient会获得好的轨迹与坏的轨迹

它提升好的轨迹出现的概率，降低坏的轨迹出现的概率

> good stuff is made more likely, bad stuff is made less likely













### 连续动作下应用Policy gradient的一个例子

![image-20240605211115383](./assets/image-20240605211115383.png)

考虑我们想要使用策略梯度让这个小型人形机器人跑步。在这种情况下，我们需要选择一种表示 $\pi$​​​​ 的方法，使其能够输出连续值动作的分布。



首先，策略梯度定理为
$$
\begin{flalign}
 \triangledown_\theta J(\theta) 



&\approx \frac{1}{N}\sum^N_{i=1}
\left(
\sum^T_{t=1}\triangledown_\theta \log \pi_\theta(a_{i,t}|s_{i,t})
\right)
\left(
\sum^T_{t=1}r(s_{i,t},a_{i,t})

\right)

\end{flalign}
$$
要在连续动作空间中使用策略梯度，我们需要将$\pi_\theta(a_t|s_t)$表示为一个Multivariate normal distribution
$$
\pi_\theta(a_t|s_t) = \mathcal{N}(f_{neural\ network}(s_t);\Sigma)
$$
其中，均值由神经网络给出，$\Sigma$是协方差矩阵，可以是学习得到的也可以是固定的。



Multivariate Normal Distribution下的对数概率为
$$
\log \pi_\theta(a_t|s_t) = -\frac{1}{2}\Vert {f(s_t)-a_t}\Vert^2_\Sigma+const
$$
对均值参数求梯度我们就得到了
$$
\triangledown_\theta \log \pi_\theta(a_t|s_t) = -\frac{1}{2}\Sigma^{-1}(f(s_t)-a_t)\frac{df}{d\theta}
$$


> 多变量正态分布(Multivariate Normal Distribution)指的是多维空间中
>
> * 每个变量都服从正态分布
> * 变量之间存在某种相关性
>
> $$
> p(x) = \frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}exp\left(-\frac{1}{2}(\mathbf{x}-\mu)^T\Sigma^{-1}(\mathbf{x}-\mu)\right)
> $$
>
> 其中
>
> * $\mathbf{x}$是一个$k$ 维随机向量
> * $\mu$ 是一个$k$ 维均值向量
> * $\Sigma$ 是一个$k\times k$​的协方差矩阵
> * $|\Sigma|$表示协方差矩阵的行列式
>
> 
>
> 在强化学习中，连续动作空间意味着动作$a$是一个实数值的向量，而不是离散的有限集合，需要一种方法来生成和优化这些实数值的动作。
>
> 假设我们有一个二维空间中的动作向量$\mathbf{a}=[a_1,a_2]$​，其分布为一个二维正态分布，我们可以定义其均值向量和协方差矩阵如下，
> $$
> \mu = 
> \begin{bmatrix}
> \mu_1
> \\
> \mu_2
> \end{bmatrix}
> ,
> \Sigma = 
> \begin{bmatrix}
> \sigma_{11}&\sigma_{12}\\
> \sigma_{21}&\sigma_{22}\\
> \end{bmatrix}
> $$
> 在这个例子中，$\mu$表示动作的期望，而协方差矩阵$\Sigma$​表示动作之间的相关性和方差
>
> * **中心位置**：均值向量 $\mu$ 表示分布的中心，即最有可能出现的动作值。在强化学习中，这意味着在给定状态下，策略认为最优的动作是什么。
>
> * **期望动作**：在每个状态 $s_t$ 下，均值向量 $\mu$是神经网络输出的期望动作。这个动作是策略在当前状态下执行的“默认”动作。
>
> * **调节性**：通过调整均值向量，可以改变动作分布的中心，从而影响策略的行为。均值向量的学习是策略优化的一部分，使得策略能够适应不同的环境和任务。
>
> 
>
> **在策略梯度中**，我们使用神经网络来输出动作的均值$\mu$和协方差矩阵$\Sigma$。
>
> 对于动作$a_t$，其服从Multivariate normal distribution
> $$
> p(\mathbf{a}) = \frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}exp\left(-\frac{1}{2}(\mathbf{a}-\mu)^T\Sigma^{-1}(\mathbf{a}-\mu)\right)
> $$
> 对数概率密度为
> $$
> \log p(\mathbf{a}) = 
> \log
> \left(
> \frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}
> \right)
> +
> \log exp
> \left(
> -\frac{1}{2}(\mathbf{a}-\mu)^T\Sigma^{-1}(\mathbf{a}-\mu)
> \right)
> $$
> 考虑这里的$log$其实是$ln$，也就是$log(exp(x))=x$
> $$
> \log p(\mathbf{a}) =\log
> \left(
> \frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}
> \right)
> -\frac{1}{2}(\mathbf{a}-\mu)^T\Sigma^{-1}(\mathbf{a}-\mu)
> $$
> 考虑
> $$
> \log
> \left(
> \frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}
> \right)
> =\log 1 - \log(2\pi)^{k/2}-\log|\Sigma|^{1/2}
> $$
> 因此对数概率密度可以简化为
> $$
> \log p(\mathbf{a}) = -\frac{k}{2}\log(2\pi)-\frac{1}{2}\log|\Sigma| - \frac{1}{2}(\mathbf{a}-\mu)^T\Sigma^{-1}(\mathbf{a}-\mu)
> $$
> **这里的$p(\mathbf{a})$实际上就是$\pi_\theta({\mathbf{a}_t|s_t})$**
>
> 因此，每次采样获得的$(s,a)$对获得的对数期望奖励为
> $$
> \color{red}\log \pi_\theta(a_t|s_t) =-\frac{k}{2}\log(2\pi)-\frac{1}{2}\log|\Sigma| - \frac{1}{2}(\mathbf{a}_t-\mu)^T\Sigma^{-1}(\mathbf{a}_t-\mu)
> $$
> 这意味着我们可以根据$a_t$获得$\theta$​的梯度
> $$
> \triangledown_\theta\log \pi_\theta(a_t|s_t) =\triangledown_\theta \left(
> - \frac{1}{2}(\mathbf{a}_t-\mu)^T\Sigma^{-1}(\mathbf{a}_t-\mu)
> \right)
> $$
> 注意，这里我们考虑的是$\mu$是网络输出，$\Sigma$可能是网络输出，因此网络梯度就是$\triangledown_\theta\log \pi_\theta(a_t|s_t)$对与网络相关参数的偏导，



![image-20240605221823211](./assets/image-20240605221823211.png)









## 3.REINFORCE

经典的RINFORCE算法就是上面推到的一个直接体现

![image-20240605163919693](./assets/image-20240605163919693.png)























## 4. 部分观察下的Policy gradient

之前我们讨论的都是状态完全可观测的情况，也就是$s_t$

现在，以视觉自动驾驶为例，其实它是一个Observation

![image-20240605222520022](./assets/image-20240605222520022.png)

事实上，通过上面的推导，observation也能推导出来一模一样的式子，其实是因为概率分布隐含于环境的关系。



此时，我们发现，在推导期望奖励的这一个阶段，马尔可夫性质并没有确实的被使用，

$\color{red}因此我们可以在partially\ observed\ MDP中使用policy\ gradient，一点都不用修改$

























